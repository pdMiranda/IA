{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bPVTxb4akHi"
      },
      "source": [
        "**Vamos experimentar agora a Rede Neural Artificial?**\n",
        "Veja:\n",
        "https://scikit-learn.org/stable/modules/neural_networks_supervised.html# "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "fpe0EYaXiIPm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: imblearn in /home/pedro/.local/lib/python3.10/site-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /home/pedro/.local/lib/python3.10/site-packages (from imblearn) (0.11.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /home/pedro/.local/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /home/pedro/.local/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.3.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /home/pedro/.local/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.11.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /home/pedro/.local/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.25.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/pedro/.local/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (3.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip -q install yellowbrick\n",
        "%pip install imblearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "ru9xg6QIaceV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('../Dados/breast-cancer.csv', sep=',',  header=0)\n",
        "df = pd.get_dummies(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definindo o pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    ('oversampler', RandomOverSampler()),\n",
        "    ('classifier', SVC())\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definindo os parâmetros para o Grid Search\n",
        "param_grid = {\n",
        "    'classifier__C': [0.1, 1, 10],\n",
        "    'classifier__kernel': ['linear', 'rbf', 'poly'],\n",
        "    'classifier__gamma': ['scale', 'auto']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[(&#x27;randomoversampler&#x27;,\n",
              "                                        RandomOverSampler()),\n",
              "                                       (&#x27;svc&#x27;, SVC())]),\n",
              "             n_jobs=-1,\n",
              "             param_grid={&#x27;svc__C&#x27;: [0.1, 1, 10],\n",
              "                         &#x27;svc__gamma&#x27;: [&#x27;scale&#x27;, &#x27;auto&#x27;],\n",
              "                         &#x27;svc__kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;, &#x27;poly&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[(&#x27;randomoversampler&#x27;,\n",
              "                                        RandomOverSampler()),\n",
              "                                       (&#x27;svc&#x27;, SVC())]),\n",
              "             n_jobs=-1,\n",
              "             param_grid={&#x27;svc__C&#x27;: [0.1, 1, 10],\n",
              "                         &#x27;svc__gamma&#x27;: [&#x27;scale&#x27;, &#x27;auto&#x27;],\n",
              "                         &#x27;svc__kernel&#x27;: [&#x27;linear&#x27;, &#x27;rbf&#x27;, &#x27;poly&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;randomoversampler&#x27;, RandomOverSampler()), (&#x27;svc&#x27;, SVC())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomOverSampler</label><div class=\"sk-toggleable__content\"><pre>RandomOverSampler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "GridSearchCV(cv=5,\n",
              "             estimator=Pipeline(steps=[('randomoversampler',\n",
              "                                        RandomOverSampler()),\n",
              "                                       ('svc', SVC())]),\n",
              "             n_jobs=-1,\n",
              "             param_grid={'svc__C': [0.1, 1, 10],\n",
              "                         'svc__gamma': ['scale', 'auto'],\n",
              "                         'svc__kernel': ['linear', 'rbf', 'poly']})"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from imblearn.pipeline import make_pipeline\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the pipeline with the oversampler step as the intermediate step\n",
        "pipeline = make_pipeline(RandomOverSampler(), SVC())\n",
        "param_grid = {\n",
        "    'svc__C': [0.1, 1, 10],\n",
        "    'svc__kernel': ['linear', 'rbf', 'poly'],\n",
        "    'svc__gamma': ['scale', 'auto']\n",
        "}\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1)\n",
        "grid_search.fit(X_treino, y_treino)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Melhores parâmetros encontrados:\n",
            "{'svc__C': 0.1, 'svc__gamma': 'scale', 'svc__kernel': 'linear'}\n"
          ]
        }
      ],
      "source": [
        "print(\"Melhores parâmetros encontrados:\")\n",
        "print(grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Salvando o modelo treinado\n",
        "with open('../Dados/breast-cancer.pickle', 'wb') as f:\n",
        "    pickle.dump((grid_search, X_teste, y_teste), f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bCxFBVNFt22"
      },
      "source": [
        "**Vamos treinar com a rede neural?**\n",
        "\n",
        "**Experimente a RNA com os parâmetros default. A rede convergiu? quantas épocas?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVW22XucaswH",
        "outputId": "2c08bca3-ed7c-4d92-c4a9-8030b70af513"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[36  0]\n",
            " [ 0 22]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       1.00      1.00      1.00        36\n",
            "        True       1.00      1.00      1.00        22\n",
            "\n",
            "    accuracy                           1.00        58\n",
            "   macro avg       1.00      1.00      1.00        58\n",
            "weighted avg       1.00      1.00      1.00        58\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "# Carregando a base de dados\n",
        "df = pd.read_csv('../Dados/breast-cancer.csv', sep=',',  header=0)\n",
        "df = pd.get_dummies(df)\n",
        "\n",
        "X = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "\n",
        "# Carregando o modelo treinado\n",
        "with open('../Dados/breast-cancer.pickle', 'rb') as f:\n",
        "    grid_search, X_teste, y_teste = pickle.load(f)\n",
        "\n",
        "# Fazendo previsões com o modelo treinado\n",
        "y_pred = grid_search.predict(X_teste)\n",
        "\n",
        "# Avaliando o desempenho do modelo\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(y_teste, y_pred))\n",
        "print(classification_report(y_teste, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6Yh14LUHfN-"
      },
      "source": [
        "**Depois execute novamente com os ajustes. Veja agora os erros a cada época.. estabeleça o verbose para true **"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QRptikHHepQ",
        "outputId": "f0106c94-1301-44e9-be60-e108bf82bebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.90303300\n",
            "Iteration 2, loss = 0.84064344\n",
            "Iteration 3, loss = 0.78799195\n",
            "Iteration 4, loss = 0.74369770\n",
            "Iteration 5, loss = 0.70653953\n",
            "Iteration 6, loss = 0.67762667\n",
            "Iteration 7, loss = 0.65382836\n",
            "Iteration 8, loss = 0.63545741\n",
            "Iteration 9, loss = 0.62071854\n",
            "Iteration 10, loss = 0.60861737\n",
            "Iteration 11, loss = 0.59912790\n",
            "Iteration 12, loss = 0.59092970\n",
            "Iteration 13, loss = 0.58313544\n",
            "Iteration 14, loss = 0.57533073\n",
            "Iteration 15, loss = 0.56805688\n",
            "Iteration 16, loss = 0.56054889\n",
            "Iteration 17, loss = 0.55205278\n",
            "Iteration 18, loss = 0.54394185\n",
            "Iteration 19, loss = 0.53450071\n",
            "Iteration 20, loss = 0.52534227\n",
            "Iteration 21, loss = 0.51595737\n",
            "Iteration 22, loss = 0.50736979\n",
            "Iteration 23, loss = 0.49779170\n",
            "Iteration 24, loss = 0.48964307\n",
            "Iteration 25, loss = 0.48147997\n",
            "Iteration 26, loss = 0.47328703\n",
            "Iteration 27, loss = 0.46523714\n",
            "Iteration 28, loss = 0.45772738\n",
            "Iteration 29, loss = 0.44968697\n",
            "Iteration 30, loss = 0.44193091\n",
            "Iteration 31, loss = 0.43433458\n",
            "Iteration 32, loss = 0.42650600\n",
            "Iteration 33, loss = 0.41851020\n",
            "Iteration 34, loss = 0.41051914\n",
            "Iteration 35, loss = 0.40249912\n",
            "Iteration 36, loss = 0.39451763\n",
            "Iteration 37, loss = 0.38687009\n",
            "Iteration 38, loss = 0.37937250\n",
            "Iteration 39, loss = 0.37208549\n",
            "Iteration 40, loss = 0.36543997\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 41, loss = 0.35907482\n",
            "Iteration 42, loss = 0.35315867\n",
            "Iteration 43, loss = 0.34742748\n",
            "Iteration 44, loss = 0.34097708\n",
            "Iteration 45, loss = 0.33410450\n",
            "Iteration 46, loss = 0.32678147\n",
            "Iteration 47, loss = 0.32014191\n",
            "Iteration 48, loss = 0.31290314\n",
            "Iteration 49, loss = 0.30673091\n",
            "Iteration 50, loss = 0.30095148\n",
            "Iteration 51, loss = 0.29520749\n",
            "Iteration 52, loss = 0.28984782\n",
            "Iteration 53, loss = 0.28482131\n",
            "Iteration 54, loss = 0.27940155\n",
            "Iteration 55, loss = 0.27377330\n",
            "Iteration 56, loss = 0.26793771\n",
            "Iteration 57, loss = 0.26211266\n",
            "Iteration 58, loss = 0.25695765\n",
            "Iteration 59, loss = 0.25195260\n",
            "Iteration 60, loss = 0.24794810\n",
            "Iteration 61, loss = 0.24379890\n",
            "Iteration 62, loss = 0.23936204\n",
            "Iteration 63, loss = 0.23465348\n",
            "Iteration 64, loss = 0.22954955\n",
            "Iteration 65, loss = 0.22467966\n",
            "Iteration 66, loss = 0.21961839\n",
            "Iteration 67, loss = 0.21497998\n",
            "Iteration 68, loss = 0.21073579\n",
            "Iteration 69, loss = 0.20643802\n",
            "Iteration 70, loss = 0.20216792\n",
            "Iteration 71, loss = 0.19797058\n",
            "Iteration 72, loss = 0.19374897\n",
            "Iteration 73, loss = 0.18970820\n",
            "Iteration 74, loss = 0.18578507\n",
            "Iteration 75, loss = 0.18179377\n",
            "Iteration 76, loss = 0.17806466\n",
            "Iteration 77, loss = 0.17448062\n",
            "Iteration 78, loss = 0.17094405\n",
            "Iteration 79, loss = 0.16758179\n",
            "Iteration 80, loss = 0.16421352\n",
            "Iteration 81, loss = 0.16075753\n",
            "Iteration 82, loss = 0.15734791\n",
            "Iteration 83, loss = 0.15419541\n",
            "Iteration 84, loss = 0.15097981\n",
            "Iteration 85, loss = 0.14786786\n",
            "Iteration 86, loss = 0.14486710\n",
            "Iteration 87, loss = 0.14183274\n",
            "Iteration 88, loss = 0.13890388\n",
            "Iteration 89, loss = 0.13619307\n",
            "Iteration 90, loss = 0.13335274\n",
            "Iteration 91, loss = 0.13072066\n",
            "Iteration 92, loss = 0.12802549\n",
            "Iteration 93, loss = 0.12525195\n",
            "Iteration 94, loss = 0.12261945\n",
            "Iteration 95, loss = 0.12021656\n",
            "Iteration 96, loss = 0.11770861\n",
            "Iteration 97, loss = 0.11527429\n",
            "Iteration 98, loss = 0.11285419\n",
            "Iteration 99, loss = 0.11064209\n",
            "Iteration 100, loss = 0.10857621\n",
            "Iteration 101, loss = 0.10664084\n",
            "Iteration 102, loss = 0.10464949\n",
            "Iteration 103, loss = 0.10263305\n",
            "Iteration 104, loss = 0.10069899\n",
            "Iteration 105, loss = 0.09865336\n",
            "Iteration 106, loss = 0.09629534\n",
            "Iteration 107, loss = 0.09439297\n",
            "Iteration 108, loss = 0.09264514\n",
            "Iteration 109, loss = 0.09103944\n",
            "Iteration 110, loss = 0.08931534\n",
            "Iteration 111, loss = 0.08754420\n",
            "Iteration 112, loss = 0.08577579\n",
            "Iteration 113, loss = 0.08406017\n",
            "Iteration 114, loss = 0.08245367\n",
            "Iteration 115, loss = 0.08090207\n",
            "Iteration 116, loss = 0.07939494\n",
            "Iteration 117, loss = 0.07799208\n",
            "Iteration 118, loss = 0.07664342\n",
            "Iteration 119, loss = 0.07522736\n",
            "Iteration 120, loss = 0.07384674\n",
            "Iteration 121, loss = 0.07232554\n",
            "Iteration 122, loss = 0.07099021\n",
            "Iteration 123, loss = 0.06953990\n",
            "Iteration 124, loss = 0.06820727\n",
            "Iteration 125, loss = 0.06698166\n",
            "Iteration 126, loss = 0.06567484\n",
            "Iteration 127, loss = 0.06441915\n",
            "Iteration 128, loss = 0.06326924\n",
            "Iteration 129, loss = 0.06214417\n",
            "Iteration 130, loss = 0.06111640\n",
            "Iteration 131, loss = 0.06013884\n",
            "Iteration 132, loss = 0.05909638\n",
            "Iteration 133, loss = 0.05800215\n",
            "Iteration 134, loss = 0.05690880\n",
            "Iteration 135, loss = 0.05582438\n",
            "Iteration 136, loss = 0.05479423\n",
            "Iteration 137, loss = 0.05387725\n",
            "Iteration 138, loss = 0.05308105\n",
            "Iteration 139, loss = 0.05214115\n",
            "Iteration 140, loss = 0.05119997\n",
            "Iteration 141, loss = 0.05028533\n",
            "Iteration 142, loss = 0.04940401\n",
            "Iteration 143, loss = 0.04854929\n",
            "Iteration 144, loss = 0.04776912\n",
            "Iteration 145, loss = 0.04696136\n",
            "Iteration 146, loss = 0.04620607\n",
            "Iteration 147, loss = 0.04544806\n",
            "Iteration 148, loss = 0.04475515\n",
            "Iteration 149, loss = 0.04410330\n",
            "Iteration 150, loss = 0.04349062\n",
            "Iteration 151, loss = 0.04290540\n",
            "Iteration 152, loss = 0.04233979\n",
            "Iteration 153, loss = 0.04171580\n",
            "Iteration 154, loss = 0.04089609\n",
            "Iteration 155, loss = 0.04012251\n",
            "Iteration 156, loss = 0.03938174\n",
            "Iteration 157, loss = 0.03877865\n",
            "Iteration 158, loss = 0.03819849\n",
            "Iteration 159, loss = 0.03772442\n",
            "Iteration 160, loss = 0.03713857\n",
            "Iteration 161, loss = 0.03655418\n",
            "Iteration 162, loss = 0.03599174\n",
            "Iteration 163, loss = 0.03542913\n",
            "Iteration 164, loss = 0.03489719\n",
            "Iteration 165, loss = 0.03435283\n",
            "Iteration 166, loss = 0.03383394\n",
            "Iteration 167, loss = 0.03334644\n",
            "Iteration 168, loss = 0.03285840\n",
            "Iteration 169, loss = 0.03237802\n",
            "Iteration 170, loss = 0.03189835\n",
            "Iteration 171, loss = 0.03142940\n",
            "Iteration 172, loss = 0.03096770\n",
            "Iteration 173, loss = 0.03051482\n",
            "Iteration 174, loss = 0.03008136\n",
            "Iteration 175, loss = 0.02964011\n",
            "Iteration 176, loss = 0.02920966\n",
            "Iteration 177, loss = 0.02879019\n",
            "Iteration 178, loss = 0.02836408\n",
            "Iteration 179, loss = 0.02795365\n",
            "Iteration 180, loss = 0.02752710\n",
            "Iteration 181, loss = 0.02708347\n",
            "Iteration 182, loss = 0.02669598\n",
            "Iteration 183, loss = 0.02641726\n",
            "Iteration 184, loss = 0.02611001\n",
            "Iteration 185, loss = 0.02576618\n",
            "Iteration 186, loss = 0.02536419\n",
            "Iteration 187, loss = 0.02491369\n",
            "Iteration 188, loss = 0.02447568\n",
            "Iteration 189, loss = 0.02408744\n",
            "Iteration 190, loss = 0.02375074\n",
            "Iteration 191, loss = 0.02346926\n",
            "Iteration 192, loss = 0.02315854\n",
            "Iteration 193, loss = 0.02283584\n",
            "Iteration 194, loss = 0.02251677\n",
            "Iteration 195, loss = 0.02218839\n",
            "Iteration 196, loss = 0.02187668\n",
            "Iteration 197, loss = 0.02155335\n",
            "Iteration 198, loss = 0.02124915\n",
            "Iteration 199, loss = 0.02096076\n",
            "Iteration 200, loss = 0.02067392\n",
            "Iteration 201, loss = 0.02040501\n",
            "Iteration 202, loss = 0.02013166\n",
            "Iteration 203, loss = 0.01986182\n",
            "Iteration 204, loss = 0.01960510\n",
            "Iteration 205, loss = 0.01937276\n",
            "Iteration 206, loss = 0.01912405\n",
            "Iteration 207, loss = 0.01887889\n",
            "Iteration 208, loss = 0.01861944\n",
            "Iteration 209, loss = 0.01836437\n",
            "Iteration 210, loss = 0.01809816\n",
            "Iteration 211, loss = 0.01782382\n",
            "Iteration 212, loss = 0.01761567\n",
            "Iteration 213, loss = 0.01741465\n",
            "Iteration 214, loss = 0.01726491\n",
            "Iteration 215, loss = 0.01709279\n",
            "Iteration 216, loss = 0.01689345\n",
            "Iteration 217, loss = 0.01666727\n",
            "Iteration 218, loss = 0.01644974\n",
            "Iteration 219, loss = 0.01622763\n",
            "Iteration 220, loss = 0.01599280\n",
            "Iteration 221, loss = 0.01577869\n",
            "Iteration 222, loss = 0.01556441\n",
            "Iteration 223, loss = 0.01535980\n",
            "Iteration 224, loss = 0.01516811\n",
            "Iteration 225, loss = 0.01497908\n",
            "Iteration 226, loss = 0.01479143\n",
            "Iteration 227, loss = 0.01460893\n",
            "Iteration 228, loss = 0.01442842\n",
            "Iteration 229, loss = 0.01425177\n",
            "Iteration 230, loss = 0.01408001\n",
            "Iteration 231, loss = 0.01390778\n",
            "Iteration 232, loss = 0.01374068\n",
            "Iteration 233, loss = 0.01357850\n",
            "Iteration 234, loss = 0.01341198\n",
            "Iteration 235, loss = 0.01324977\n",
            "Iteration 236, loss = 0.01308817\n",
            "Iteration 237, loss = 0.01293357\n",
            "Iteration 238, loss = 0.01278102\n",
            "Iteration 239, loss = 0.01263466\n",
            "Iteration 240, loss = 0.01249562\n",
            "Iteration 241, loss = 0.01234912\n",
            "Iteration 242, loss = 0.01220734\n",
            "Iteration 243, loss = 0.01207012\n",
            "Iteration 244, loss = 0.01193160\n",
            "Iteration 245, loss = 0.01179717\n",
            "Iteration 246, loss = 0.01166331\n",
            "Iteration 247, loss = 0.01153189\n",
            "Iteration 248, loss = 0.01139583\n",
            "Iteration 249, loss = 0.01126483\n",
            "Iteration 250, loss = 0.01113580\n",
            "Iteration 251, loss = 0.01101093\n",
            "Iteration 252, loss = 0.01089064\n",
            "Iteration 253, loss = 0.01077154\n",
            "Iteration 254, loss = 0.01065145\n",
            "Iteration 255, loss = 0.01053197\n",
            "Iteration 256, loss = 0.01041152\n",
            "Iteration 257, loss = 0.01029658\n",
            "Iteration 258, loss = 0.01017223\n",
            "Iteration 259, loss = 0.01005904\n",
            "Iteration 260, loss = 0.00994739\n",
            "Iteration 261, loss = 0.00983404\n",
            "Iteration 262, loss = 0.00972591\n",
            "Iteration 263, loss = 0.00962725\n",
            "Iteration 264, loss = 0.00951796\n",
            "Iteration 265, loss = 0.00941378\n",
            "Iteration 266, loss = 0.00930446\n",
            "Iteration 267, loss = 0.00920034\n",
            "Iteration 268, loss = 0.00910700\n",
            "Iteration 269, loss = 0.00902120\n",
            "Iteration 270, loss = 0.00893803\n",
            "Iteration 271, loss = 0.00885156\n",
            "Iteration 272, loss = 0.00876220\n",
            "Iteration 273, loss = 0.00866550\n",
            "Iteration 274, loss = 0.00857005\n",
            "Iteration 275, loss = 0.00847342\n",
            "Iteration 276, loss = 0.00838929\n",
            "Iteration 277, loss = 0.00830425\n",
            "Iteration 278, loss = 0.00823677\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-9 {color: black;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(max_iter=1000, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" checked><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(max_iter=1000, verbose=True)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "MLPClassifier(max_iter=1000, verbose=True)"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "modelo = MLPClassifier(max_iter=1000, verbose=True)\n",
        "modelo.fit(X_treino, y_treino)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PQZVMhEJXOC"
      },
      "source": [
        "**Faça outras alterações nos parâmetros**\n",
        "\n",
        "**4 entradas - 3 neurônios - 3 neurônios - 1**\n",
        "\n",
        "**Veja SoftMax para problemas multiclasse**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6Q1RssrJU9z",
        "outputId": "802b6904-3a64-4a71-fd3b-221c672f3524"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 0.80973237\n",
            "Iteration 2, loss = 0.76133222\n",
            "Iteration 3, loss = 0.72114859\n",
            "Iteration 4, loss = 0.68769304\n",
            "Iteration 5, loss = 0.65972109\n",
            "Iteration 6, loss = 0.63869239\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 7, loss = 0.61963017\n",
            "Iteration 8, loss = 0.60471962\n",
            "Iteration 9, loss = 0.59210896\n",
            "Iteration 10, loss = 0.58195562\n",
            "Iteration 11, loss = 0.57207116\n",
            "Iteration 12, loss = 0.56326978\n",
            "Iteration 13, loss = 0.55525526\n",
            "Iteration 14, loss = 0.54748214\n",
            "Iteration 15, loss = 0.53969801\n",
            "Iteration 16, loss = 0.53192640\n",
            "Iteration 17, loss = 0.52428791\n",
            "Iteration 18, loss = 0.51656166\n",
            "Iteration 19, loss = 0.50908717\n",
            "Iteration 20, loss = 0.50158037\n",
            "Iteration 21, loss = 0.49410906\n",
            "Iteration 22, loss = 0.48647714\n",
            "Iteration 23, loss = 0.47906392\n",
            "Iteration 24, loss = 0.47185003\n",
            "Iteration 25, loss = 0.46439220\n",
            "Iteration 26, loss = 0.45776645\n",
            "Iteration 27, loss = 0.45039039\n",
            "Iteration 28, loss = 0.44372263\n",
            "Iteration 29, loss = 0.43700373\n",
            "Iteration 30, loss = 0.43051438\n",
            "Iteration 31, loss = 0.42476230\n",
            "Iteration 32, loss = 0.41894002\n",
            "Iteration 33, loss = 0.41297614\n",
            "Iteration 34, loss = 0.40717298\n",
            "Iteration 35, loss = 0.40140421\n",
            "Iteration 36, loss = 0.39531728\n",
            "Iteration 37, loss = 0.38885222\n",
            "Iteration 38, loss = 0.38238891\n",
            "Iteration 39, loss = 0.37601666\n",
            "Iteration 40, loss = 0.36965659\n",
            "Iteration 41, loss = 0.36332817\n",
            "Iteration 42, loss = 0.35813337\n",
            "Iteration 43, loss = 0.35299334\n",
            "Iteration 44, loss = 0.34789503\n",
            "Iteration 45, loss = 0.34207719\n",
            "Iteration 46, loss = 0.33599840\n",
            "Iteration 47, loss = 0.33004616\n",
            "Iteration 48, loss = 0.32426896\n",
            "Iteration 49, loss = 0.31879776\n",
            "Iteration 50, loss = 0.31343958\n",
            "Iteration 51, loss = 0.30806163\n",
            "Iteration 52, loss = 0.30238675\n",
            "Iteration 53, loss = 0.29700762\n",
            "Iteration 54, loss = 0.29158453\n",
            "Iteration 55, loss = 0.28627325\n",
            "Iteration 56, loss = 0.28098169\n",
            "Iteration 57, loss = 0.27559507\n",
            "Iteration 58, loss = 0.27028815\n",
            "Iteration 59, loss = 0.26484778\n",
            "Iteration 60, loss = 0.25953263\n",
            "Iteration 61, loss = 0.25440494\n",
            "Iteration 62, loss = 0.24936006\n",
            "Iteration 63, loss = 0.24427051\n",
            "Iteration 64, loss = 0.23973165\n",
            "Iteration 65, loss = 0.23518390\n",
            "Iteration 66, loss = 0.23106793\n",
            "Iteration 67, loss = 0.22655038\n",
            "Iteration 68, loss = 0.22192718\n",
            "Iteration 69, loss = 0.21707434\n",
            "Iteration 70, loss = 0.21275510\n",
            "Iteration 71, loss = 0.20805105\n",
            "Iteration 72, loss = 0.20342793\n",
            "Iteration 73, loss = 0.19937252\n",
            "Iteration 74, loss = 0.19589644\n",
            "Iteration 75, loss = 0.19240215\n",
            "Iteration 76, loss = 0.18874542\n",
            "Iteration 77, loss = 0.18473095\n",
            "Iteration 78, loss = 0.18067173\n",
            "Iteration 79, loss = 0.17652200\n",
            "Iteration 80, loss = 0.17303502\n",
            "Iteration 81, loss = 0.16936781\n",
            "Iteration 82, loss = 0.16593666\n",
            "Iteration 83, loss = 0.16248557\n",
            "Iteration 84, loss = 0.15916320\n",
            "Iteration 85, loss = 0.15584658\n",
            "Iteration 86, loss = 0.15243833\n",
            "Iteration 87, loss = 0.14933004\n",
            "Iteration 88, loss = 0.14623089\n",
            "Iteration 89, loss = 0.14339079\n",
            "Iteration 90, loss = 0.14069412\n",
            "Iteration 91, loss = 0.13779799\n",
            "Iteration 92, loss = 0.13475365\n",
            "Iteration 93, loss = 0.13161561\n",
            "Iteration 94, loss = 0.12849151\n",
            "Iteration 95, loss = 0.12566066\n",
            "Iteration 96, loss = 0.12293991\n",
            "Iteration 97, loss = 0.12045431\n",
            "Iteration 98, loss = 0.11812705\n",
            "Iteration 99, loss = 0.11591648\n",
            "Iteration 100, loss = 0.11362508\n",
            "Iteration 101, loss = 0.11130442\n",
            "Iteration 102, loss = 0.10889535\n",
            "Iteration 103, loss = 0.10648625\n",
            "Iteration 104, loss = 0.10414178\n",
            "Iteration 105, loss = 0.10239271\n",
            "Iteration 106, loss = 0.10088562\n",
            "Iteration 107, loss = 0.09927078\n",
            "Iteration 108, loss = 0.09718657\n",
            "Iteration 109, loss = 0.09488799\n",
            "Iteration 110, loss = 0.09245911\n",
            "Iteration 111, loss = 0.09038377\n",
            "Iteration 112, loss = 0.08856434\n",
            "Iteration 113, loss = 0.08682984\n",
            "Iteration 114, loss = 0.08506838\n",
            "Iteration 115, loss = 0.08332318\n",
            "Iteration 116, loss = 0.08160631\n",
            "Iteration 117, loss = 0.08001286\n",
            "Iteration 118, loss = 0.07856952\n",
            "Iteration 119, loss = 0.07733895\n",
            "Iteration 120, loss = 0.07601956\n",
            "Iteration 121, loss = 0.07449723\n",
            "Iteration 122, loss = 0.07295246\n",
            "Iteration 123, loss = 0.07144655\n",
            "Iteration 124, loss = 0.07008097\n",
            "Iteration 125, loss = 0.06870123\n",
            "Iteration 126, loss = 0.06740094\n",
            "Iteration 127, loss = 0.06617701\n",
            "Iteration 128, loss = 0.06491356\n",
            "Iteration 129, loss = 0.06382218\n",
            "Iteration 130, loss = 0.06263739\n",
            "Iteration 131, loss = 0.06137796\n",
            "Iteration 132, loss = 0.06017251\n",
            "Iteration 133, loss = 0.05904378\n",
            "Iteration 134, loss = 0.05796825\n",
            "Iteration 135, loss = 0.05692981\n",
            "Iteration 136, loss = 0.05592318\n",
            "Iteration 137, loss = 0.05499690\n",
            "Iteration 138, loss = 0.05406910\n",
            "Iteration 139, loss = 0.05317012\n",
            "Iteration 140, loss = 0.05221255\n",
            "Iteration 141, loss = 0.05133144\n",
            "Iteration 142, loss = 0.05041811\n",
            "Iteration 143, loss = 0.04958408\n",
            "Iteration 144, loss = 0.04875336\n",
            "Iteration 145, loss = 0.04796703\n",
            "Iteration 146, loss = 0.04715501\n",
            "Iteration 147, loss = 0.04634948\n",
            "Iteration 148, loss = 0.04552671\n",
            "Iteration 149, loss = 0.04475012\n",
            "Iteration 150, loss = 0.04401534\n",
            "Iteration 151, loss = 0.04330150\n",
            "Iteration 152, loss = 0.04267379\n",
            "Iteration 153, loss = 0.04201606\n",
            "Iteration 154, loss = 0.04125548\n",
            "Iteration 155, loss = 0.04046108\n",
            "Iteration 156, loss = 0.03968792\n",
            "Iteration 157, loss = 0.03899692\n",
            "Iteration 158, loss = 0.03840192\n",
            "Iteration 159, loss = 0.03782991\n",
            "Iteration 160, loss = 0.03724958\n",
            "Iteration 161, loss = 0.03667308\n",
            "Iteration 162, loss = 0.03614351\n",
            "Iteration 163, loss = 0.03545577\n",
            "Iteration 164, loss = 0.03470994\n",
            "Iteration 165, loss = 0.03400408\n",
            "Iteration 166, loss = 0.03336060\n",
            "Iteration 167, loss = 0.03276388\n",
            "Iteration 168, loss = 0.03218452\n",
            "Iteration 169, loss = 0.03163091\n",
            "Iteration 170, loss = 0.03110158\n",
            "Iteration 171, loss = 0.03058610\n",
            "Iteration 172, loss = 0.03009366\n",
            "Iteration 173, loss = 0.02961241\n",
            "Iteration 174, loss = 0.02915094\n",
            "Iteration 175, loss = 0.02866843\n",
            "Iteration 176, loss = 0.02812802\n",
            "Iteration 177, loss = 0.02758654\n",
            "Iteration 178, loss = 0.02717694\n",
            "Iteration 179, loss = 0.02686838\n",
            "Iteration 180, loss = 0.02657113\n",
            "Iteration 181, loss = 0.02627789\n",
            "Iteration 182, loss = 0.02585224\n",
            "Iteration 183, loss = 0.02531687\n",
            "Iteration 184, loss = 0.02479398\n",
            "Iteration 185, loss = 0.02435415\n",
            "Iteration 186, loss = 0.02398236\n",
            "Iteration 187, loss = 0.02361822\n",
            "Iteration 188, loss = 0.02326820\n",
            "Iteration 189, loss = 0.02289865\n",
            "Iteration 190, loss = 0.02252319\n",
            "Iteration 191, loss = 0.02216907\n",
            "Iteration 192, loss = 0.02181462\n",
            "Iteration 193, loss = 0.02147752\n",
            "Iteration 194, loss = 0.02114383\n",
            "Iteration 195, loss = 0.02085196\n",
            "Iteration 196, loss = 0.02055936\n",
            "Iteration 197, loss = 0.02027267\n",
            "Iteration 198, loss = 0.01999305\n",
            "Iteration 199, loss = 0.01970339\n",
            "Iteration 200, loss = 0.01942990\n",
            "Iteration 201, loss = 0.01913069\n",
            "Iteration 202, loss = 0.01885969\n",
            "Iteration 203, loss = 0.01860466\n",
            "Iteration 204, loss = 0.01834213\n",
            "Iteration 205, loss = 0.01810753\n",
            "Iteration 206, loss = 0.01787187\n",
            "Iteration 207, loss = 0.01763911\n",
            "Iteration 208, loss = 0.01740159\n",
            "Iteration 209, loss = 0.01716134\n",
            "Iteration 210, loss = 0.01693941\n",
            "Iteration 211, loss = 0.01670492\n",
            "Iteration 212, loss = 0.01646794\n",
            "Iteration 213, loss = 0.01622571\n",
            "Iteration 214, loss = 0.01603274\n",
            "Iteration 215, loss = 0.01581164\n",
            "Iteration 216, loss = 0.01560778\n",
            "Iteration 217, loss = 0.01540025\n",
            "Iteration 218, loss = 0.01520950\n",
            "Iteration 219, loss = 0.01502319\n",
            "Iteration 220, loss = 0.01484387\n",
            "Iteration 221, loss = 0.01465885\n",
            "Iteration 222, loss = 0.01448452\n",
            "Iteration 223, loss = 0.01431180\n",
            "Iteration 224, loss = 0.01414627\n",
            "Iteration 225, loss = 0.01397730\n",
            "Iteration 226, loss = 0.01381951\n",
            "Iteration 227, loss = 0.01365161\n",
            "Iteration 228, loss = 0.01348589\n",
            "Iteration 229, loss = 0.01331389\n",
            "Iteration 230, loss = 0.01314161\n",
            "Iteration 231, loss = 0.01297119\n",
            "Iteration 232, loss = 0.01284836\n",
            "Iteration 233, loss = 0.01271899\n",
            "Iteration 234, loss = 0.01259327\n",
            "Iteration 235, loss = 0.01245183\n",
            "Iteration 236, loss = 0.01230458\n",
            "Iteration 237, loss = 0.01215368\n",
            "Iteration 238, loss = 0.01199371\n",
            "Iteration 239, loss = 0.01184101\n",
            "Iteration 240, loss = 0.01169929\n",
            "Iteration 241, loss = 0.01156798\n",
            "Iteration 242, loss = 0.01144291\n",
            "Iteration 243, loss = 0.01132374\n",
            "Iteration 244, loss = 0.01121043\n",
            "Iteration 245, loss = 0.01110446\n",
            "Iteration 246, loss = 0.01099079\n",
            "Iteration 247, loss = 0.01087562\n",
            "Iteration 248, loss = 0.01074869\n",
            "Iteration 249, loss = 0.01062981\n",
            "Iteration 250, loss = 0.01051688\n",
            "Iteration 251, loss = 0.01040953\n",
            "Iteration 252, loss = 0.01030321\n",
            "Iteration 253, loss = 0.01020782\n",
            "Iteration 254, loss = 0.01011301\n",
            "Iteration 255, loss = 0.01001565\n",
            "Iteration 256, loss = 0.00992214\n",
            "Iteration 257, loss = 0.00982526\n",
            "Iteration 258, loss = 0.00972423\n",
            "Iteration 259, loss = 0.00961792\n",
            "Iteration 260, loss = 0.00951700\n",
            "Iteration 261, loss = 0.00941429\n",
            "Iteration 262, loss = 0.00932043\n",
            "Iteration 263, loss = 0.00922729\n",
            "Iteration 264, loss = 0.00913890\n",
            "Iteration 265, loss = 0.00904775\n",
            "Iteration 266, loss = 0.00896166\n",
            "Iteration 267, loss = 0.00887483\n",
            "Iteration 268, loss = 0.00879409\n",
            "Iteration 269, loss = 0.00871719\n",
            "Iteration 270, loss = 0.00864497\n",
            "Iteration 271, loss = 0.00856987\n",
            "Iteration 272, loss = 0.00849058\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-10 {color: black;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(max_iter=1000, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" checked><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(max_iter=1000, verbose=True)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "MLPClassifier(max_iter=1000, verbose=True)"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rede_neural = MLPClassifier(max_iter=1000, verbose=True, tol=0.00000000000001, solver = 'adam', activation = 'relu', hidden_layer_sizes = 9)\n",
        "modelo.fit(X_treino, y_treino)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq-S4o3IczVP"
      },
      "source": [
        "\n",
        "\n",
        "> **Vamos testar o modelo?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "1q9nsbSjdu23"
      },
      "outputs": [],
      "source": [
        "previsoes = modelo.predict(X_teste)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0PlSJE8fAUL",
        "outputId": "501c2371-84dd-4c64-f216-be2b03707888"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([False,  True, False,  True, False,  True, False,  True, False,\n",
              "       False,  True, False, False, False,  True, False, False,  True,\n",
              "       False, False, False, False,  True, False,  True, False,  True,\n",
              "       False, False, False, False, False, False, False,  True, False,\n",
              "       False, False, False,  True,  True, False, False,  True, False,\n",
              "       False, False,  True,  True,  True,  True, False,  True, False,\n",
              "       False,  True, False, False])"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "previsoes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjWziqc5fV8m"
      },
      "source": [
        "\n",
        "\n",
        "> **Será se o modelo acertou?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q92H3KOtfN5E",
        "outputId": "864bf803-aaec-436b-ee87-2bd4956fc72e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([False,  True,  True,  True, False,  True, False,  True, False,\n",
              "       False,  True, False, False, False,  True, False, False,  True,\n",
              "       False, False, False, False,  True, False,  True, False,  True,\n",
              "       False, False, False, False, False, False, False,  True, False,\n",
              "       False, False, False,  True,  True, False, False,  True, False,\n",
              "       False, False,  True,  True,  True,  True, False,  True, False,\n",
              "       False,  True, False,  True])"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ9MxYOIfmwv",
        "outputId": "d81573e1-45aa-4ea6-d922-c25e1a623343"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9655172413793104"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "accuracy_score(y_teste,previsoes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3D5bvushr9W",
        "outputId": "45328eb3-d2be-4d4c-c5e1-248e262d7d9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[36,  0],\n",
              "       [ 2, 20]])"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from yellowbrick.classifier import ConfusionMatrix\n",
        "confusion_matrix(y_teste, previsoes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "wX15YT-7j-c9",
        "outputId": "db8b53f9-e72b-4257-c955-a229ab056d41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9655172413793104"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x550 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "cm = ConfusionMatrix(modelo)\n",
        "cm.fit(X_treino, y_treino)\n",
        "cm.score(X_teste, y_teste)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIixOPw1kw-z",
        "outputId": "5b4735c0-0e3f-4981-b422-f4a5d25f5c76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.95      1.00      0.97        36\n",
            "        True       1.00      0.91      0.95        22\n",
            "\n",
            "    accuracy                           0.97        58\n",
            "   macro avg       0.97      0.95      0.96        58\n",
            "weighted avg       0.97      0.97      0.97        58\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_teste, previsoes))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csRtFY7lKr0N"
      },
      "source": [
        "**Veja como implementar o backpropagation em python:**\n",
        "https://www.askpython.com/python/examples/backpropagation-in-python\n",
        "https://www.deeplearningbook.com.br/algoritmo-backpropagation-em-python/\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
