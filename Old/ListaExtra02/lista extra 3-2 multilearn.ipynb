{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregar os dados de treino\n",
    "train_data = pd.read_csv('train.csv')\n",
    "X_train = train_data['comment_text']  # A coluna com os comentários\n",
    "y_train = train_data.iloc[:, 2:]      # As colunas com as etiquetas de toxicidade\n",
    "\n",
    "# Carregar os dados de teste\n",
    "test_data = pd.read_csv('test.csv')\n",
    "test_labels = pd.read_csv('test_labels.csv')\n",
    "\n",
    "# Filtrar as entradas de teste onde as etiquetas não são -1\n",
    "test_labels = test_labels[test_labels['toxic'] != -1]\n",
    "X_test = test_data[test_data['id'].isin(test_labels['id'])]['comment_text']\n",
    "y_test = test_labels.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pedro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/pedro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "\n",
    "    # Usando SnowballStemmer\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "    stemmed_tokens = []\n",
    "    for token in filtered_tokens:\n",
    "        try:\n",
    "            stemmed_token = stemmer.stem(token)\n",
    "            stemmed_tokens.append(stemmed_token)\n",
    "        except RecursionError:\n",
    "            # Caso um erro de recursão ocorra, simplesmente use o token não modificado\n",
    "            print(\"erro\");\n",
    "            stemmed_tokens.append(token)\n",
    "\n",
    "    return \" \".join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = [preprocess_text(text) for text in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_preprocessed = [preprocess_text(text) for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_transformed = vectorizer.fit_transform(X_train_preprocessed)\n",
    "X_test_transformed = vectorizer.transform(X_test_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skmultilearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mskmultilearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproblem_transform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BinaryRelevance\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Inicializando o classificador de Regressão Logística com n_jobs=-1\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'skmultilearn'"
     ]
    }
   ],
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Inicializando o classificador de Regressão Logística com n_jobs=-1\n",
    "base_classifier = LogisticRegression(n_jobs=-1)\n",
    "\n",
    "# Inicializando o classificador Binary Relevance com o classificador base\n",
    "br_classifier = BinaryRelevance(classifier=base_classifier, require_dense=[False, True])\n",
    "\n",
    "# Treinando o modelo\n",
    "br_classifier.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Fazendo previsões\n",
    "y_pred = br_classifier.predict(X_test_transformed)\n",
    "\n",
    "# Avaliando o modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Acurácia: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'br_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpatches\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Rectangle\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Define y_pred variable here\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mbr_classifier\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test_transformed)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Gerando matrizes de confusão para cada rótulo\u001b[39;00m\n\u001b[1;32m     10\u001b[0m confusion_matrices \u001b[38;5;241m=\u001b[39m multilabel_confusion_matrix(y_test, y_pred)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'br_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Gerando matrizes de confusão para cada rótulo\n",
    "confusion_matrices = multilabel_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Nomes das classes (rótulos)\n",
    "class_names = y_test.columns\n",
    "\n",
    "# Inicializando a figura para a grade de matrizes de confusão\n",
    "num_rows = int(np.ceil(len(class_names) / 3))\n",
    "fig, axarr = plt.subplots(num_rows, 3, figsize=(8, 7))  \n",
    "axes = axarr.flatten()\n",
    "\n",
    "# Preenchendo a grade com as matrizes de confusão\n",
    "for i, matrix in enumerate(confusion_matrices):\n",
    "    axes[i].imshow(matrix, cmap=\"Blues\", aspect='auto')\n",
    "    axes[i].set_title(class_names[i], fontsize=14)\n",
    "    \n",
    "    # Adicionar anotações numéricas\n",
    "    for j in range(matrix.shape[0]):\n",
    "        for k in range(matrix.shape[1]):\n",
    "            axes[i].text(k, j, str(matrix[j, k]), ha='center', va='center', fontsize=16)\n",
    "    \n",
    "    axes[i].axis('off')\n",
    "    rect = Rectangle((0,0), 1, 1, edgecolor='black', facecolor='none', transform=axes[i].transAxes, linewidth=3)\n",
    "    axes[i].add_patch(rect)\n",
    "\n",
    "    # Adicionando rótulos de linha e coluna\n",
    "    if i % 3 == 0:  # Para a primeira coluna\n",
    "        for j, cls in enumerate(['No', 'Yes']):  # Substitua por nomes mais apropriados das classes se necessário\n",
    "            axes[i].text(-0.6, j, cls, ha='right', va='center', color='black', transform=axes[i].transData)\n",
    "\n",
    "    for j, cls in enumerate(['No', 'Yes']):  # Substitua por nomes mais apropriados das classes se necessário\n",
    "        axes[i].text(j, 1.9, cls, ha='center', va='bottom', color='black', transform=axes[i].transData)\n",
    "\n",
    "# Ocultando os eixos não utilizados\n",
    "for i in range(len(confusion_matrices), len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Classe  Precisão    Recall  F1-Score\n",
      "0           toxic  0.648167  0.717241  0.680957\n",
      "1    severe_toxic  0.384083  0.302452  0.338415\n",
      "2         obscene  0.741423  0.632349  0.682556\n",
      "3          threat  0.415584  0.151659  0.222222\n",
      "4          insult  0.714627  0.523198  0.604111\n",
      "5   identity_hate  0.674242  0.250000  0.364754\n",
      "6  Acurácia Geral  0.895870  0.895870  0.895870\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Calculando as métricas para cada rótulo\n",
    "precision_per_class = precision_score(y_test, y_pred, average=None)\n",
    "recall_per_class = recall_score(y_test, y_pred, average=None)\n",
    "f1_per_class = f1_score(y_test, y_pred, average=None)\n",
    "\n",
    "# Acurácia geral (não por classe, pois a acurácia é uma métrica global)\n",
    "accuracy_overall = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Criando um DataFrame com as métricas\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Classe': y_test.columns,\n",
    "    'Precisão': precision_per_class,\n",
    "    'Recall': recall_per_class,\n",
    "    'F1-Score': f1_per_class\n",
    "})\n",
    "\n",
    "# Criando uma linha para a acurácia geral\n",
    "accuracy_df = pd.DataFrame({'Classe': ['Acurácia Geral'], 'Precisão': [accuracy_overall], 'Recall': [accuracy_overall], 'F1-Score': [accuracy_overall]})\n",
    "\n",
    "# Concatenando com a tabela de métricas\n",
    "metrics_df = pd.concat([metrics_df, accuracy_df], ignore_index=True)\n",
    "\n",
    "# Exibindo a tabela\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
